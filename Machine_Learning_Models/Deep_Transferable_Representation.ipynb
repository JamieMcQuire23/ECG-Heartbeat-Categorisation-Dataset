{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Transferable_Representation.ipynb",
      "provenance": [],
      "mount_file_id": "1R4KPwdmV_E1brlrYt8oHbWu-6isLRIIP",
      "authorship_tag": "ABX9TyO2sEfn8lRRulvVjZUB3oQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamieMcQuire23/ECG-Heartbeat-Categorisation-Dataset/blob/master/Deep_Transferable_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGSDQgcwLXul",
        "colab_type": "text"
      },
      "source": [
        "# Deep Transferable Representation \n",
        "\n",
        "* This notebook is going to provide a PyTorch implementation of the deep transferable network from https://arxiv.org/pdf/1805.00794.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpY6_WfHK6x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWCAWsFeL7NX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9209ea23-93ad-416a-e6f6-9d294d1d1cee"
      },
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f72b29a2d70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS1mjF8hLtBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read in the mit dataset\n",
        "mit_train = pd.read_csv(\"/content/drive/My Drive/ECG_HeartBeat_Pytorch/Data/heartbeat/mitbih_train.csv\",header=None)\n",
        "mit_test = pd.read_csv(\"/content/drive/My Drive/ECG_HeartBeat_Pytorch/Data/heartbeat/mitbih_test.csv\",header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GWxHjFMMzpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to the correct form\n",
        "\n",
        "train_vals = mit_train.values\n",
        "X_train = train_vals[:, :-1]\n",
        "Y_train = train_vals[:, -1].astype(int)\n",
        "\n",
        "test_vals = mit_test.values\n",
        "X_test = test_vals[:,:-1]\n",
        "Y_test = test_vals[:,:-1].astype(int)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIv-LwAINXL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the training and validation set from the training data set\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=0.2,random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWIi_tydOBDE",
        "colab_type": "text"
      },
      "source": [
        "# Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jSYNvzNOsoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (1,187)\n",
        "num_classes = 5\n",
        "batch_size = 100\n",
        "learning_rate = 1e-3\n",
        "n_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDNfjcwlTqpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DTR_Block(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "\n",
        "    super(DTR_Block,self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv1d(in_channels=32,out_channels=32,kernel_size=5,padding=2)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.conv2 = nn.Conv1d(in_channels=32,out_channels=32,kernel_size=5,padding=2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool = nn.MaxPool1d(kernel_size=5,padding=2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    residual = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.relu1(out)\n",
        "    out = self.conv2(out)\n",
        "    out += residual\n",
        "    out = self.relu2(out)\n",
        "    out = self.pool(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFtEgBWBMwHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DTR(nn.Module):\n",
        "\n",
        "  def __init__(self,block,batch_size,input_dim,num_classes=5):\n",
        "    super(DTR, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=1,out_channels=32,kernel_size=5,padding=2)\n",
        "    self.layer1 = self.make_layer(block,input_dim,batch_size)\n",
        "    self.layer2 = self.make_layer(block,input_dim,batch_size)\n",
        "    self.layer3 = self.make_layer(block,input_dim,batch_size)\n",
        "    self.layer4 = self.make_layer(block,input_dim,batch_size)\n",
        "    self.layer5 = self.make_layer(block,input_dim,batch_size)\n",
        "    self.fc1 = nn.Linear(32,64)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(64,num_classes)\n",
        "\n",
        "  def make_layer(self,block,input_dim,batch_size):\n",
        "    layers = []\n",
        "    layers.append(block(in_channels=32,out_channels=32))\n",
        "    return nn.Sequential(*layers)     \n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = self.layer5(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu1(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9KOm--5YYe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "2e451886-37c3-4020-e88e-ad8eee614939"
      },
      "source": [
        "dtr_model = DTR(DTR_Block,batch_size=batch_size,input_dim=187)\n",
        "dtr_model.cuda()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DTR(\n",
              "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  (layer1): Sequential(\n",
              "    (0): DTR_Block(\n",
              "      (conv1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu1): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu2): ReLU()\n",
              "      (pool): MaxPool1d(kernel_size=5, stride=5, padding=2, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): DTR_Block(\n",
              "      (conv1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu1): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu2): ReLU()\n",
              "      (pool): MaxPool1d(kernel_size=5, stride=5, padding=2, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): DTR_Block(\n",
              "      (conv1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu1): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu2): ReLU()\n",
              "      (pool): MaxPool1d(kernel_size=5, stride=5, padding=2, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): DTR_Block(\n",
              "      (conv1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu1): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu2): ReLU()\n",
              "      (pool): MaxPool1d(kernel_size=5, stride=5, padding=2, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (layer5): Sequential(\n",
              "    (0): DTR_Block(\n",
              "      (conv1): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu1): ReLU()\n",
              "      (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "      (relu2): ReLU()\n",
              "      (pool): MaxPool1d(kernel_size=5, stride=5, padding=2, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T06Bk14x5dv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mit_train = pd.read_csv(\"/content/drive/My Drive/ECG_HeartBeat_Pytorch/Data/heartbeat/mitbih_train.csv\",header=None)\n",
        "mit_test = pd.read_csv(\"/content/drive/My Drive/ECG_HeartBeat_Pytorch/Data/heartbeat/mitbih_test.csv\",header=None)\n",
        "\n",
        "mit_train, mit_val = train_test_split(mit_train,test_size=0.2,random_state=seed)\n",
        "\n",
        "\n",
        "train_tensor = torch.Tensor(mit_train.values)\n",
        "val_tensor = torch.Tensor(mit_val.values)\n",
        "test_tensor = torch.Tensor(mit_test.values)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_tensor,batch_size=batch_size,\n",
        "                                          shuffle=False,num_workers=2)\n",
        "\n",
        "validationloader = torch.utils.data.DataLoader(val_tensor,batch_size=batch_size,\n",
        "                                               shuffle=False,num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_tensor,batch_size=batch_size,\n",
        "                                          shuffle=False,num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMl0rBbveFcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    \n",
        "    #Loss function\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    #Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return(loss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "magzRNIgaOnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from torch.autograd import Variable\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate, trainloader, validationloader):\n",
        "    \n",
        "    #Print all of the hyperparameters of the training iteration:\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    n_batches = len(trainloader)\n",
        "    \n",
        "    #Create our loss and optimizer functions\n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    \n",
        "    #Time for printing\n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    #Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        print_every = n_batches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, data in enumerate(trainloader):\n",
        "            \n",
        "            inputs = data[:, :-1]\n",
        "            labels = data[:, -1].long()\n",
        "            sizes = inputs.shape[0]\n",
        "            #inputs form batch_size, features, time_steps\n",
        "            inputs = inputs.reshape(sizes,1,187)\n",
        "\n",
        "            #Wrap them in a Variable object\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            \n",
        "            #Set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #Forward pass, backward pass, optimize\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            #Print statistics\n",
        "            running_loss += loss_size.item()\n",
        "            total_train_loss += loss_size.item()\n",
        "            \n",
        "            #Print every 10th batch of an epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
        "                #Reset running loss and time\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            \n",
        "        #At the end of the epoch, do a pass on the validation set\n",
        "        total_val_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for data in validationloader:\n",
        "\n",
        "            inputs = data[:, :-1]\n",
        "            labels = data[:, -1].long()\n",
        "            sizes = inputs.shape[0]\n",
        "            inputs = inputs.reshape(sizes,1,187)\n",
        "\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            \n",
        "            #Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            #Forward pass\n",
        "            val_outputs = net(inputs)\n",
        "            val_loss_size = loss(val_outputs, labels)\n",
        "            total_val_loss += val_loss_size.item()\n",
        "\n",
        "            _, predicted = torch.max(val_outputs.data,1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum()\n",
        "            \n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(validationloader)))\n",
        "\n",
        "        print(\"Accuracy of the network on the validation data: %d %%\" % (100 * correct/total))\n",
        "        \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCQGURp4am8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbbcaafb-06af-4be7-c050-5e0d8ff7faa6"
      },
      "source": [
        "trainNet(dtr_model, batch_size, n_epochs, learning_rate, trainloader, validationloader)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 100\n",
            "epochs= 10\n",
            "learning_rate= 0.001\n",
            "==============================\n",
            "Epoch 1, 10% \t train_loss: 0.10 took: 1.75s\n",
            "Epoch 1, 20% \t train_loss: 0.10 took: 1.67s\n",
            "Epoch 1, 30% \t train_loss: 0.09 took: 1.69s\n",
            "Epoch 1, 40% \t train_loss: 0.10 took: 1.66s\n",
            "Epoch 1, 50% \t train_loss: 0.08 took: 1.67s\n",
            "Epoch 1, 60% \t train_loss: 0.09 took: 1.67s\n",
            "Epoch 1, 70% \t train_loss: 0.10 took: 1.68s\n",
            "Epoch 1, 81% \t train_loss: 0.09 took: 1.67s\n",
            "Epoch 1, 91% \t train_loss: 0.11 took: 1.66s\n",
            "Validation loss = 0.09\n",
            "Accuracy of the network on the validation data: 97 %\n",
            "Epoch 2, 10% \t train_loss: 0.07 took: 1.74s\n",
            "Epoch 2, 20% \t train_loss: 0.08 took: 1.66s\n",
            "Epoch 2, 30% \t train_loss: 0.07 took: 1.68s\n",
            "Epoch 2, 40% \t train_loss: 0.08 took: 1.67s\n",
            "Epoch 2, 50% \t train_loss: 0.07 took: 1.66s\n",
            "Epoch 2, 60% \t train_loss: 0.08 took: 1.67s\n",
            "Epoch 2, 70% \t train_loss: 0.08 took: 1.65s\n",
            "Epoch 2, 81% \t train_loss: 0.07 took: 1.66s\n",
            "Epoch 2, 91% \t train_loss: 0.09 took: 1.66s\n",
            "Validation loss = 0.08\n",
            "Accuracy of the network on the validation data: 97 %\n",
            "Epoch 3, 10% \t train_loss: 0.06 took: 1.74s\n",
            "Epoch 3, 20% \t train_loss: 0.08 took: 1.66s\n",
            "Epoch 3, 30% \t train_loss: 0.06 took: 1.67s\n",
            "Epoch 3, 40% \t train_loss: 0.07 took: 1.67s\n",
            "Epoch 3, 50% \t train_loss: 0.06 took: 1.68s\n",
            "Epoch 3, 60% \t train_loss: 0.07 took: 1.67s\n",
            "Epoch 3, 70% \t train_loss: 0.07 took: 1.66s\n",
            "Epoch 3, 81% \t train_loss: 0.06 took: 1.67s\n",
            "Epoch 3, 91% \t train_loss: 0.07 took: 1.67s\n",
            "Validation loss = 0.09\n",
            "Accuracy of the network on the validation data: 97 %\n",
            "Epoch 4, 10% \t train_loss: 0.06 took: 1.74s\n",
            "Epoch 4, 20% \t train_loss: 0.06 took: 1.68s\n",
            "Epoch 4, 30% \t train_loss: 0.05 took: 1.68s\n",
            "Epoch 4, 40% \t train_loss: 0.06 took: 1.66s\n",
            "Epoch 4, 50% \t train_loss: 0.06 took: 1.66s\n",
            "Epoch 4, 60% \t train_loss: 0.06 took: 1.68s\n",
            "Epoch 4, 70% \t train_loss: 0.06 took: 1.68s\n",
            "Epoch 4, 81% \t train_loss: 0.05 took: 1.68s\n",
            "Epoch 4, 91% \t train_loss: 0.06 took: 1.67s\n",
            "Validation loss = 0.07\n",
            "Accuracy of the network on the validation data: 97 %\n",
            "Epoch 5, 10% \t train_loss: 0.05 took: 1.75s\n",
            "Epoch 5, 20% \t train_loss: 0.06 took: 1.65s\n",
            "Epoch 5, 30% \t train_loss: 0.05 took: 1.65s\n",
            "Epoch 5, 40% \t train_loss: 0.05 took: 1.68s\n",
            "Epoch 5, 50% \t train_loss: 0.04 took: 1.69s\n",
            "Epoch 5, 60% \t train_loss: 0.05 took: 1.66s\n",
            "Epoch 5, 70% \t train_loss: 0.06 took: 1.66s\n",
            "Epoch 5, 81% \t train_loss: 0.05 took: 1.66s\n",
            "Epoch 5, 91% \t train_loss: 0.06 took: 1.66s\n",
            "Validation loss = 0.07\n",
            "Accuracy of the network on the validation data: 98 %\n",
            "Epoch 6, 10% \t train_loss: 0.04 took: 1.74s\n",
            "Epoch 6, 20% \t train_loss: 0.05 took: 1.66s\n",
            "Epoch 6, 30% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 6, 40% \t train_loss: 0.05 took: 1.66s\n",
            "Epoch 6, 50% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 6, 60% \t train_loss: 0.05 took: 1.67s\n",
            "Epoch 6, 70% \t train_loss: 0.06 took: 1.69s\n",
            "Epoch 6, 81% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 6, 91% \t train_loss: 0.05 took: 1.66s\n",
            "Validation loss = 0.07\n",
            "Accuracy of the network on the validation data: 97 %\n",
            "Epoch 7, 10% \t train_loss: 0.04 took: 1.74s\n",
            "Epoch 7, 20% \t train_loss: 0.04 took: 1.66s\n",
            "Epoch 7, 30% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 7, 40% \t train_loss: 0.05 took: 1.66s\n",
            "Epoch 7, 50% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 7, 60% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 7, 70% \t train_loss: 0.05 took: 1.68s\n",
            "Epoch 7, 81% \t train_loss: 0.04 took: 1.69s\n",
            "Epoch 7, 91% \t train_loss: 0.05 took: 1.69s\n",
            "Validation loss = 0.07\n",
            "Accuracy of the network on the validation data: 98 %\n",
            "Epoch 8, 10% \t train_loss: 0.04 took: 1.76s\n",
            "Epoch 8, 20% \t train_loss: 0.04 took: 1.65s\n",
            "Epoch 8, 30% \t train_loss: 0.04 took: 1.66s\n",
            "Epoch 8, 40% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 8, 50% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 8, 60% \t train_loss: 0.04 took: 1.67s\n",
            "Epoch 8, 70% \t train_loss: 0.04 took: 1.66s\n",
            "Epoch 8, 81% \t train_loss: 0.04 took: 1.66s\n",
            "Epoch 8, 91% \t train_loss: 0.04 took: 1.65s\n",
            "Validation loss = 0.07\n",
            "Accuracy of the network on the validation data: 98 %\n",
            "Epoch 9, 10% \t train_loss: 0.03 took: 1.75s\n",
            "Epoch 9, 20% \t train_loss: 0.04 took: 1.65s\n",
            "Epoch 9, 30% \t train_loss: 0.04 took: 1.65s\n",
            "Epoch 9, 40% \t train_loss: 0.05 took: 1.66s\n",
            "Epoch 9, 50% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 9, 60% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 9, 70% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 9, 81% \t train_loss: 0.03 took: 1.66s\n",
            "Epoch 9, 91% \t train_loss: 0.04 took: 1.66s\n",
            "Validation loss = 0.06\n",
            "Accuracy of the network on the validation data: 98 %\n",
            "Epoch 10, 10% \t train_loss: 0.03 took: 1.74s\n",
            "Epoch 10, 20% \t train_loss: 0.04 took: 1.65s\n",
            "Epoch 10, 30% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 10, 40% \t train_loss: 0.04 took: 1.68s\n",
            "Epoch 10, 50% \t train_loss: 0.03 took: 1.67s\n",
            "Epoch 10, 60% \t train_loss: 0.03 took: 1.66s\n",
            "Epoch 10, 70% \t train_loss: 0.04 took: 1.66s\n",
            "Epoch 10, 81% \t train_loss: 0.03 took: 1.67s\n",
            "Epoch 10, 91% \t train_loss: 0.04 took: 1.67s\n",
            "Validation loss = 0.07\n",
            "Accuracy of the network on the validation data: 98 %\n",
            "Training finished, took 175.45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Iw4IeqBuUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75708a45-a4a2-4ced-e8b5-b0be0de92ec5"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in testloader:\n",
        "  inputs = data[:, :-1]\n",
        "  labels = data[:, -1].long()\n",
        "  sizes = inputs.shape[0]\n",
        "  inputs = inputs.reshape(sizes,1,187)\n",
        "\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    inputs = inputs.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "  outputs = dtr_model(inputs)\n",
        "  _, predicted = torch.max(outputs.data,1)\n",
        "  total += labels.size(0)\n",
        "  correct += (predicted == labels).sum()\n",
        "\n",
        "print(\"Accuracy of the network on the test data: %d %%\" % (100 * correct/total))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test data: 98 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}